{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:SteelBlue; font-size:200%; line-height:1.5\">Определение тональности комментариев</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно обучить модель классифицировать комментарии на позитивные и негативные. Целевая метрика - f1. Нужно получить значение на тестовой выборке не ниже 0.75. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:SteelBlue; font-size:200%; line-height:1.5\">Введение</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В работе опирался на [Ваш первый BERT: иллюстрированное руководство](https://github.com/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb), [Smart batchin tutorial](https://colab.research.google.com/drive/1Er23iD96x_SzmRG8md1kVggbmz0su_Q5#scrollTo=Sa7tgw2-Pttq), [Lena VoitaNLP course]( https://lena-voita.github.io/nlp_course/text_classification.html), [kaggle mini course Natural Language Processing](https://www.kaggle.com/learn/natural-language-processing), [Подходы лемматизации с примерами на Python](https://webdevblog.ru/podhody-lemmatizacii-s-primerami-v-python/)\n",
    "\n",
    "Попробовал spaCy и BERT (и DistilBERT) от transformers, и можно было ещё попробовать реализации word2vec от spaCy и разные языковые модели. Возможно, можно было как-то улучшить показатели комбинациями vectorizer-pca-gradient boosting. Не стал включать в тетрадку, т.к. предварительные результаты были хуже логистической регрессии, тюнинг бустинга на таком наборе данных очень затратен по времени, а логистическая регрессия вполне справилась с задачей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:SteelBlue; font-size:200%; line-height:1.5\">1. Загрузка данных, общая информация</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re #регулярные выражения для очистки текста\n",
    "import random\n",
    "import torch #для тензоров BERT\n",
    "import csv #для записи эмбеддингов\n",
    "\n",
    "from tqdm import notebook #для отслеживания прогресса\n",
    "from datetime import datetime #для оценки времени лемматизации\n",
    "\n",
    "#BERT от transformers\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "\n",
    "#spaCy\n",
    "import spacy\n",
    "from spacy.pipeline import TextCategorizer\n",
    "from spacy.util import minibatch\n",
    "\n",
    "#natural language toolkit\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#векторизация, метрики, пайплайн, подготовка данных, модели от sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция для вывода отчёта о прогнозе модели\n",
    "def print_report(true, predicted):\n",
    "    \n",
    "    report = classification_report(true, predicted)\n",
    "    roc_auc = roc_auc_score(true, predicted)\n",
    "\n",
    "    cm = confusion_matrix(true, predicted)\n",
    "    cm_df = pd.DataFrame(data=cm, index=['Actual NO', 'Actual YES'], columns = ['Predicted NO', 'Predicted YES'])\n",
    "    display(cm_df)\n",
    "    print(f'{\"report\":-^60}\\n\\n', report)\n",
    "    print( 'ROC_AUC:', round(roc_auc,2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция для удаления из текста никнеймов и проч.\n",
    "def text_cleaner(raw):\n",
    "    return \" \".join(re.sub(r'[^a-zA-Z\\']', ' ', raw).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция возвращает кортеж с тегом «part-of-speech»\n",
    "#взята с https://webdevblog.ru/podhody-lemmatizacii-s-primerami-v-python/\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция для лемматизации на базе nltk\n",
    "def ntlk_text_lemmatizer(row):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_row = ''\n",
    "    for w in nltk.word_tokenize(row):\n",
    "        lemmatized_row += lemmatizer.lemmatize(w, get_wordnet_pos(w)) + ' '\n",
    "        \n",
    "    return lemmatized_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция для лемматизации на базе spacy\n",
    "def spacy_text_lemmatizer(raw):\n",
    "    doc = nlp(raw)\n",
    "    return \" \".join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция для кросс-валидации с векторизацией\n",
    "def cross_val_func(model, vectorizer, features, target):\n",
    "    pipe = \\\n",
    "    Pipeline(steps = [('vectorizer', vectorizer),\n",
    "                      ('model', model)])\n",
    "    cv_score = cross_val_score(pipe, features, target, cv=5, scoring='f1')\n",
    "    print('Cross-validation results, f1', cv_score)\n",
    "    print('Cross-validation mean score, f1', cv_score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link='datasets/ML/'\n",
    "df = pd.read_csv(link + 'toxic_comments.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159571 non-null  object\n",
      " 1   toxic   159571 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143346\n",
       "1     16225\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные - объёмный (почти 160 тыс. объектов) датасет с сильно разбалансированными классами. Без дублей и пропусков. Требуется очистка текста от спецсимволов и проч."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].astype('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(text_cleaner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//ремарка:\n",
    "\n",
    "Уже после обучения BERT и spaCy на полном датасете я выяснил, что часть (1243) комментариев содержала только цифры; вероятно, цифры не стоило очищать. Однако для BERT датасет (полный, а не тот, который я привёл в тетрадке) был получен уже без цифр, поэтому не стал переделывать это, чтобы можно было сравнить результаты. В принципе, можно было поработать с текстом, проверить на пустые комментарии, комментарии только с цифрами и т.д. Возможно, стоило оставлять скобки и двоеточия и выделить эмодзи. Варианты для предобработки есть. Если развивать проект, можно экспериментировать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation Why the edits made under my userna...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww He matches this background colour I'm se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man I'm really not trying to edit war It's...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>More I can't make any real suggestions on impr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You sir are my hero Any chance you remember wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation Why the edits made under my userna...      0\n",
       "1  D'aww He matches this background colour I'm se...      0\n",
       "2  Hey man I'm really not trying to edit war It's...      0\n",
       "3  More I can't make any real suggestions on impr...      0\n",
       "4  You sir are my hero Any chance you remember wh...      0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:SteelBlue; font-size:200%; line-height:1.5\">2. Классический подход: Naive Bayes Classifier, Logistic Regression</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:DodgerBlue\">2.1 Лемматизация</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравню два варианта лемматизации: на базе Natural Language Toolkit и на базе spaCy. Для spaCy возьму самую лёгкую модель [en_core_web_sm](https://spacy.io/models/en#en_core_web_sm) (English multi-task CNN trained on OntoNotes. Assigns context-specific token vectors, POS tags, dependency parse and named entities). Для nltk - wordnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный текст: Explanation Why the edits made under my username Hardcore Metallica Fan were reverted They weren't vandalisms just closure on some GAs after I voted at New York Dolls FAC And please don't remove the template from the talk page since I'm retired now \n",
      "\n",
      "Лемматизированный текст spacy: explanation why the edit make under -PRON- username Hardcore Metallica Fan be revert -PRON- be not vandalism just closure on some gas after -PRON- vote at New York Dolls FAC and please do not remove the template from the talk page since -PRON- be retire now \n",
      "\n",
      "Лемматизированный текст ntlk: Explanation Why the edits make under my username Hardcore Metallica Fan be revert They be n't vandalism just closure on some GAs after I vote at New York Dolls FAC And please do n't remove the template from the talk page since I 'm retire now  \n",
      "\n",
      "Исходный текст: Fair use rationale for Image Wonju jpg Thanks for uploading Image Wonju jpg I notice the image page specifies that the image is being used under fair use but there is no explanation or rationale as to why its use in Wikipedia articles constitutes fair use In addition to the boilerplate fair use template you must also write out on the image description page a specific explanation or rationale for why using this image in each article is consistent with fair use Please go to the image description page and edit it to include a fair use rationale If you have uploaded other fair use media consider checking that you have specified the fair use rationale on those pages too You can find a list of 'image' pages you have edited by clicking on the my contributions link it is located at the very top of any Wikipedia page when you are logged in and then selecting Image from the dropdown box Note that any fair use images uploaded after May and lacking such an explanation will be deleted one week after they have been uploaded as described on criteria for speedy deletion If you have any questions please ask them at the Media copyright questions page Thank you talk contribs Unspecified source for Image Wonju jpg Thanks for uploading Image Wonju jpg I noticed that the file's description page currently doesn't specify who created the content so the copyright status is unclear If you did not create this file yourself then you will need to specify the owner of the copyright If you obtained it from a website then a link to the website from which it was taken together with a restatement of that website's terms of use of its content is usually sufficient information However if the copyright holder is different from the website's publisher then their copyright should also be acknowledged As well as adding the source please add a proper copyright licensing tag if the file doesn't have one already If you created took the picture audio or video then the tag can be used to release it under the GFDL If you believe the media meets the criteria at Wikipedia Fair use use a tag such as or one of the other tags listed at Wikipedia Image copyright tags Fair use See Wikipedia Image copyright tags for the full list of copyright tags that you can use If you have uploaded other files consider checking that you have specified their source and tagged them too You can find a list of files you have uploaded by following this link Unsourced and untagged images may be deleted one week after they have been tagged as described on criteria for speedy deletion If the image is copyrighted under a non free license per Wikipedia Fair use then the image will be deleted hours after If you have any questions please ask them at the Media copyright questions page Thank you talk contribs \n",
      "\n",
      "Лемматизированный текст spacy: fair use rationale for Image Wonju jpg thank for upload Image Wonju jpg -PRON- notice the image page specify that the image be be use under fair use but there be no explanation or rationale as to why -PRON- use in Wikipedia article constitute fair use in addition to the boilerplate fair use template -PRON- must also write out on the image description page a specific explanation or rationale for why use this image in each article be consistent with fair use please go to the image description page and edit -PRON- to include a fair use rationale if -PRON- have upload other fair use medium consider check that -PRON- have specify the fair use rationale on those page too -PRON- can find a list of ' image ' page -PRON- have edit by click on the -PRON- contribution link -PRON- be locate at the very top of any Wikipedia page when -PRON- be log in and then select image from the dropdown box note that any fair use image upload after May and lack such an explanation will be delete one week after -PRON- have be upload as describe on criterion for speedy deletion if -PRON- have any question please ask -PRON- at the Media copyright question page thank -PRON- talk contribs unspecified source for Image Wonju jpg thank for upload Image Wonju jpg -PRON- notice that the file 's description page currently do not specify who create the content so the copyright status be unclear if -PRON- do not create this file -PRON- then -PRON- will need to specify the owner of the copyright if -PRON- obtain -PRON- from a website then a link to the website from which -PRON- be take together with a restatement of that website 's term of use of -PRON- content be usually sufficient information however if the copyright holder be different from the website 's publisher then -PRON- copyright should also be acknowledge as well as add the source please add a proper copyright licensing tag if the file do not have one already if -PRON- create take the picture audio or video then the tag can be use to release -PRON- under the GFDL if -PRON- believe the medium meet the criterion at Wikipedia Fair use use a tag such as or one of the other tag list at Wikipedia Image copyright tag fair use See Wikipedia Image copyright tag for the full list of copyright tag that -PRON- can use if -PRON- have upload other file consider check that -PRON- have specify -PRON- source and tag -PRON- too -PRON- can find a list of file -PRON- have upload by follow this link unsourced and untagged image may be delete one week after -PRON- have be tag as describe on criterion for speedy deletion if the image be copyright under a non free license per Wikipedia Fair use then the image will be delete hour after if -PRON- have any question please ask -PRON- at the Media copyright question page thank -PRON- talk contribs \n",
      "\n",
      "Лемматизированный текст ntlk: Fair use rationale for Image Wonju jpg Thanks for upload Image Wonju jpg I notice the image page specifies that the image be be use under fair use but there be no explanation or rationale a to why it use in Wikipedia article constitutes fair use In addition to the boilerplate fair use template you must also write out on the image description page a specific explanation or rationale for why use this image in each article be consistent with fair use Please go to the image description page and edit it to include a fair use rationale If you have uploaded other fair use medium consider check that you have specify the fair use rationale on those page too You can find a list of 'image ' page you have edit by click on the my contribution link it be locate at the very top of any Wikipedia page when you be log in and then select Image from the dropdown box Note that any fair use image uploaded after May and lack such an explanation will be delete one week after they have be uploaded a described on criterion for speedy deletion If you have any question please ask them at the Media copyright question page Thank you talk contribs Unspecified source for Image Wonju jpg Thanks for upload Image Wonju jpg I notice that the file 's description page currently do n't specify who create the content so the copyright status be unclear If you do not create this file yourself then you will need to specify the owner of the copyright If you obtain it from a website then a link to the website from which it be take together with a restatement of that website 's term of use of it content be usually sufficient information However if the copyright holder be different from the website 's publisher then their copyright should also be acknowledge As well a add the source please add a proper copyright licensing tag if the file do n't have one already If you create take the picture audio or video then the tag can be use to release it under the GFDL If you believe the medium meet the criterion at Wikipedia Fair use use a tag such a or one of the other tag list at Wikipedia Image copyright tag Fair use See Wikipedia Image copyright tag for the full list of copyright tag that you can use If you have uploaded other file consider check that you have specify their source and tag them too You can find a list of file you have uploaded by follow this link Unsourced and untagged image may be delete one week after they have be tag a described on criterion for speedy deletion If the image be copyright under a non free license per Wikipedia Fair use then the image will be delete hour after If you have any question please ask them at the Media copyright question page Thank you talk contribs  \n",
      "\n",
      "Исходный текст: However the Moonlite edit noted by golden daph was me on optus Wake up wikkis So funny \n",
      "\n",
      "Лемматизированный текст spacy: however the Moonlite edit note by golden daph be -PRON- on optus Wake up wikkis so funny \n",
      "\n",
      "Лемматизированный текст ntlk: However the Moonlite edit note by golden daph be me on optus Wake up wikkis So funny  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [0, 10, 100]:\n",
    "    print(\"Исходный текст:\", df['text'].values[i], '\\n')\n",
    "    print(\"Лемматизированный текст spacy:\", spacy_text_lemmatizer(df['text'].values[i]), '\\n')\n",
    "    print(\"Лемматизированный текст ntlk:\", ntlk_text_lemmatizer(df['text'].values[i]), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy использует -PRON - в качестве леммы для личных местоимений (т.к. нет четкой базовой формы личного местоимения). Можно оставлять оригинальный текст для местоимений, но пока оставлю так. Похоже, лучше обрабатывает варианты с апострофами. Проверю, какой метод быстрее работает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemm_time_spent(df, lemmatizer):\n",
    "    time_spent = []\n",
    "    for i in range(0,6):\n",
    "        start = datetime.now()\n",
    "        df.loc[0:99, 'text'].apply(lemmatizer)\n",
    "        end = datetime.now()\n",
    "        time_spent.append((end - start).total_seconds())\n",
    "    return time_spent   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ntlk_time_spent = lemm_time_spent(df, ntlk_text_lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spacy_time_spent = lemm_time_spent(df, spacy_text_lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntlk_text_lemmatizer\n",
      "время на обработку первых 100 объектов датасета, секунд:\n",
      "3.305 +/- 0.062\n",
      "\n",
      "spacy_text_lemmatizer\n",
      "время на обработку первых 100 объектов датасета, секунд:\n",
      "1.259 +/- 0.158\n"
     ]
    }
   ],
   "source": [
    "print('ntlk_text_lemmatizer')\n",
    "print('время на обработку первых 100 объектов датасета, секунд:')\n",
    "print(np.mean(ntlk_time_spent).round(3), '+/-', 2*np.std(ntlk_time_spent).round(3))\n",
    "print('\\nspacy_text_lemmatizer')\n",
    "print('время на обработку первых 100 объектов датасета, секунд:')\n",
    "print(np.mean(spacy_time_spent).round(3), '+/-', 2*np.std(spacy_time_spent).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy в 3 раза быстрее. Применю её. Код лемматизации закомментил, т.к. процесс идёт долго. Полученный результат залил на Яндекс.Диск. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# df.loc[:, 'lemm_text'] = 0\n",
    "\n",
    "# for i in notebook.tqdm( range(0, (len(df) // 100)) ):\n",
    "#     df.loc[i*100:(i+1)*100, 'lemm_text'] = df.loc[i*100:(i+1)*100, 'text'].apply(spacy_text_lemmatizer)\n",
    "    \n",
    "# df.loc[100*(len(df)//100):, 'lemm_text'] = df.loc[100*(len(df)//100):, 'text'].apply(ntlk_text_lemmatizer)\n",
    "# df['lemm_text'].to_csv('lemm_text_spacy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'lemm_text'] = pd.read_csv('https://getfile.dokpub.com/yandex/get/https://yadi.sk/d/jsOV_zCK4xfRRA')['lemm_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После лемматизации появляется 6 nan, связанных с отсутствием оригинального текста. Их заполню пустыми строками, удалять не буду, чтобы не путаться при создании и сравнении моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'lemm_text'] = df.loc[:, 'lemm_text'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['lemm_text']\n",
    "y = df['toxic']\n",
    "\n",
    "#убираю, чтобы не занимать память\n",
    "df.drop(columns='lemm_text', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, stratify = y, random_state = 27)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((135635,), (23936,))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:DodgerBlue\">2.2 Векторизация признаков и обучение моделей. Выбор модели</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Набор стоп-слов получил объединением *spacy_stop и nltk_stop* (на одном spacy_stop код падает с ошибкой). Для векторизации использовал *TfidfVectorizer, CountVectorizer и HashingVectorizer*. Последний самый долгий и у него самое низкое значение метрики. Код с ним закомментил. Выполнил сравнение и выбрал комбинацию модель-векторайзер.\n",
    "\n",
    "Модели взял LogisticRegression() и ComplementNB() (согласно документации, [хорошо подходит](https://scikit-learn.org/stable/modules/naive_bayes.html#complement-naive-bayes) для несбалансированных классов и задач классификации текста) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получение набора стоп-слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_stop = spacy.lang.en.stop_words.STOP_WORDS\n",
    "nltk_stop = set(nltk_stopwords.words('english'))\n",
    "final_stop = \\\n",
    "spacy_stop | nltk_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Векторайзеры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfid_vectorizer = TfidfVectorizer(stop_words=final_stop, ngram_range=(1, 1))\n",
    "count_vectorizer = CountVectorizer(stop_words=final_stop, ngram_range=(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модели (гиперпараметры <code>alpha</code> и <code>C</code> пробовал менять, оставил те, что предварительно показались оптимальными; касательно <code>solver</code> и <code>max_iter</code>, опытным путём выяснил, что <code>'sag' и 'saga'</code> достигают достаточной точности примерно после 100 эпох, но не сходятся. <code>'saga'</code> достигает большей точности. <code>'lbfgs'</code> лучше сходится, но на это нужно иногда несколько сотен итераций):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_model = ComplementNB(alpha=.3)\n",
    "logreg_model = LogisticRegression(solver='lbfgs', \n",
    "                                  class_weight='balanced', \n",
    "                                  max_iter=10000, C=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценка работы моделей кросс-валидацией с tfid_vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    5.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation results, f1 [0.76153717 0.77290045 0.76984127 0.77732794 0.75514573]\n",
      "Cross-validation mean score, f1 0.7673505099920955\n",
      "Wall time: 59.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cross_val_func(logreg_model, tfid_vectorizer, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation results, f1 [0.66911508 0.68988566 0.69085834 0.67536534 0.67745909]\n",
      "Cross-validation mean score, f1 0.6805367022710268\n",
      "Wall time: 31.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cross_val_func(bayes_model, tfid_vectorizer, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   53.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   47.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   48.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   53.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   47.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation results, f1 [0.74461934 0.75199235 0.7574021  0.75915721 0.74101177]\n",
      "Cross-validation mean score, f1 0.7508365546777567\n",
      "Wall time: 4min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#с HashingVectorizer хуже сходимость, нужно 350-400 итераций\n",
    "vectorizer = HashingVectorizer(stop_words=final_stop, ngram_range=(1, 1))\n",
    "cross_val_func(logreg_model, vectorizer, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   44.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   46.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   40.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   41.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   46.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation results, f1 [0.75034014 0.76464578 0.76645073 0.75953737 0.74800881]\n",
      "Cross-validation mean score, f1 0.7577965660831911\n",
      "Wall time: 4min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#с count vectorizer сходится за 1500-1700 итераций\n",
    "cross_val_func(logreg_model, count_vectorizer, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation results, f1 [0.65053843 0.66188734 0.6616341  0.66408922 0.6518671 ]\n",
      "Cross-validation mean score, f1 0.6580032384619126\n",
      "Wall time: 30.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cross_val_func(bayes_model, count_vectorizer, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfVectorizer и LogisticRegression составляют лучшую пару по результату."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:DodgerBlue\">2.3 Настройка TfidfVectorizer + LogisticRegression</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TfidfVectorizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавлю <code>ngram_range=(1, 2)</code> (сколько раз рассматриваемое слово встречается в контексте другого. диапазон размеров контекстного окна - от 1 до 2 слов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   47.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   45.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   35.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   54.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   24.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation results, f1 [0.77973799 0.78812088 0.78709677 0.78595259 0.7721059 ]\n",
      "Cross-validation mean score, f1 0.7826028262859853\n",
      "Wall time: 4min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vectorizer = TfidfVectorizer(stop_words=final_stop, ngram_range=(1, 2))\n",
    "cross_val_func(logreg_model, vectorizer, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стало лучше. Пробовал также ngram_range=(2, 2) и ngram_range=(1, 3), с ними хуже. Закомментил."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# vectorizer = TfidfVectorizer(stop_words=final_stop, ngram_range=(2, 2))\n",
    "# cross_val_func(logreg_model, vectorizer, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# vectorizer = TfidfVectorizer(stop_words=final_stop, ngram_range=(1, 3))\n",
    "# cross_val_func(logreg_model, vectorizer, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшее значение r1 на <code>ngram_range=(1, 2)</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LogisticRegression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрю расширенный отчёт по прогнозу модели, сделаю валидационную выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vectorizer = TfidfVectorizer(stop_words=final_stop, ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_val, y_tr, y_val =  train_test_split(X_train, y_train, test_size=0.15, stratify=y_train, random_state = 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = final_vectorizer.fit_transform(X_tr)\n",
    "X_val = final_vectorizer.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настройка регуляризации при автоматической балансировке весов классов. При ослаблении регуляризации <code>saga</code> сходится примерно на 100-120 эпохах, примерно в 3 раза быстрее, чем <code>lbfgs</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 109 epochs took 30 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   30.2s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted NO</th>\n",
       "      <th>Predicted YES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual NO</th>\n",
       "      <td>17847</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual YES</th>\n",
       "      <td>439</td>\n",
       "      <td>1630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Predicted NO  Predicted YES\n",
       "Actual NO          17847            430\n",
       "Actual YES           439           1630"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------report---------------------------\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98     18277\n",
      "           1       0.79      0.79      0.79      2069\n",
      "\n",
      "    accuracy                           0.96     20346\n",
      "   macro avg       0.88      0.88      0.88     20346\n",
      "weighted avg       0.96      0.96      0.96     20346\n",
      "\n",
      "ROC_AUC: 0.88\n",
      "f1_score 0.7895374182610801\n",
      "Wall time: 30.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(solver='saga', \n",
    "                                  class_weight='balanced', \n",
    "                                  max_iter=10000, C=20, verbose=1)\n",
    "\n",
    "model.fit(X_tr, y_tr)\n",
    "y_val_predicted = model.predict(X_val)\n",
    "print_report(y_val, y_val_predicted)\n",
    "print('f1_score', f1_score(y_val, y_val_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробую ещё ручную балансировку классов. Прошёл от 1:1 до 4:1, выяснил, что по метрике оптимально 3.5:1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 103 epochs took 29 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   28.8s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted NO</th>\n",
       "      <th>Predicted YES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual NO</th>\n",
       "      <td>17983</td>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual YES</th>\n",
       "      <td>515</td>\n",
       "      <td>1554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Predicted NO  Predicted YES\n",
       "Actual NO          17983            294\n",
       "Actual YES           515           1554"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------report---------------------------\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98     18277\n",
      "           1       0.84      0.75      0.79      2069\n",
      "\n",
      "    accuracy                           0.96     20346\n",
      "   macro avg       0.91      0.87      0.89     20346\n",
      "weighted avg       0.96      0.96      0.96     20346\n",
      "\n",
      "ROC_AUC: 0.87\n",
      "f1_score 0.7934643860097014\n",
      "Wall time: 29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(solver='saga', \n",
    "                           max_iter=10000, \n",
    "                           C=20, verbose=1, class_weight = {1:3.5, 0:1})\n",
    "model.fit(X_tr, y_tr)\n",
    "y_val_predicted = model.predict(X_val)\n",
    "print_report(y_val, y_val_predicted)\n",
    "print('f1_score', f1_score(y_val, y_val_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во втором варианте заданная метрика примерно такая же, но значительно выше точность (почти в два раза меньше ложных срабатываний). При этом число верно обнаруженных токсичных комментариев ниже, примерно на 5%. Не ясно, что именно важно: баланс точности и полноты или метрика. Всё же во втором варианте метрика чуть выше. Прогоню оба варианта на кросс-валидации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 465 epochs took 120 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.0min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 107 epochs took 28 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   27.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 105 epochs took 27 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   27.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 109 epochs took 28 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   28.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 119 epochs took 31 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   30.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation results, f1 [0.78935939 0.79488104 0.79364506 0.79270738 0.78422861]\n",
      "Cross-validation mean score, f1 0.79096429780116\n",
      "{1: 3.5, 0: 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 104 epochs took 27 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   26.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 106 epochs took 28 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   27.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 108 epochs took 28 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   28.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 107 epochs took 27 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   26.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 106 epochs took 26 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   26.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation results, f1 [0.79185693 0.79484751 0.79417329 0.79289263 0.78760221]\n",
      "Cross-validation mean score, f1 0.7922745106465395\n"
     ]
    }
   ],
   "source": [
    "for _weight in ['balanced', {1:3.5, 0:1}]:\n",
    "    print(_weight)\n",
    "    model = LogisticRegression(solver='saga', \n",
    "                               max_iter=10000, \n",
    "                               C=20, \n",
    "                               verbose=1, \n",
    "                               class_weight = _weight)\n",
    "    vectorizer = TfidfVectorizer(stop_words=final_stop, ngram_range=(1, 2))\n",
    "    cross_val_func(model, vectorizer, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Второй вариант <code>class_weight = {1:3.5, 0:1}</code> на кросс-валидации показывает лучший результат и лучше сходится. Возьму его."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_tr, y_tr, X_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:DodgerBlue\">2.4 Финальный пайплайн и проверка на тестовой выборке</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = LogisticRegression(solver='saga', \n",
    "                                 max_iter=10000, \n",
    "                                 C=20, \n",
    "                                 verbose=1, \n",
    "                                 class_weight = {1:3.5, 0:1}\n",
    "                                )\n",
    "\n",
    "final_vectorizer = TfidfVectorizer(stop_words=final_stop, \n",
    "                                   ngram_range=(1, 2))\n",
    "\n",
    "final_pipe = \\\n",
    "Pipeline(steps=[\n",
    "    ('vectorizer', final_vectorizer),\n",
    "    ('model', final_model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 106 epochs took 35 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   34.8s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted NO</th>\n",
       "      <th>Predicted YES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual NO</th>\n",
       "      <td>21089</td>\n",
       "      <td>413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual YES</th>\n",
       "      <td>607</td>\n",
       "      <td>1827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Predicted NO  Predicted YES\n",
       "Actual NO          21089            413\n",
       "Actual YES           607           1827"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------report---------------------------\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98     21502\n",
      "           1       0.82      0.75      0.78      2434\n",
      "\n",
      "    accuracy                           0.96     23936\n",
      "   macro avg       0.89      0.87      0.88     23936\n",
      "weighted avg       0.96      0.96      0.96     23936\n",
      "\n",
      "ROC_AUC: 0.87\n",
      "f1_score 0.7817715019255456\n"
     ]
    }
   ],
   "source": [
    "final_pipe.fit(X_train, y_train)\n",
    "y_test_predicted = final_pipe.predict(X_test)\n",
    "print_report(y_test, y_test_predicted)\n",
    "final_f1 = f1_score(y_test, y_test_predicted)\n",
    "print('f1_score', final_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат на тесте по целевой метрике чуть хуже чем на кросс-валидации, но не значительно. Целевое значение метрики достигнуто, **r1 = 0.78 > 0.75**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:SteelBlue; font-size:200%; line-height:1.5\">3. spaCy TextCategorizer</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На базе миникурса kaggle и документации spaCy попробовал использовать spaCy для классификации. Поскольку всё запускал на cpu на локальной машине, обучение идёт очень долго. Здесь приведу процесс обучения и прогноза на маленькой (1%, около 1500 записей) выборке и небольшом количестве эпох. \n",
    "\n",
    "Архитектуру модели взял <code>\"simple_cnn\"</code> (*A neural network model where token vectors are calculated using a CNN. The vectors are mean pooled and used as features in a feed-forward network. This architecture is usually less accurate than the ensemble, but runs faster.*)\n",
    "\n",
    "\n",
    "Модель взял самую простую из основного набора <code>\"en_core_web_sm\"</code> (*English multi-task CNN trained on OntoNotes. Assigns context-specific token vectors, POS tags, dependency parse and named entities.*). В [пайп](https://spacy.io/usage/processing-pipelines) добавил категоризатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#получаю стратифицированную выборку 1% из датасета\n",
    "_, X_scaPy, _, y_scaPy = \\\n",
    "train_test_split( df['text'], df['toxic'], test_size = 0.01, stratify = df['toxic'], random_state = 27 )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scapy, X_test_scapy, y_train_scapy, y_test_scapy = \\\n",
    "train_test_split( X_scaPy, y_scaPy, test_size = 0.2, stratify = y_scaPy, random_state = 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#загружаю модель\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "#создаю пайп-категоризатор\n",
    "textcat = nlp.create_pipe(\"textcat\", config={\"architecture\": \"simple_cnn\"})\n",
    "#и добавляю его в модеь\n",
    "nlp.add_pipe(textcat)\n",
    "#добавляю в категоризатор метки классов\n",
    "textcat.add_label('0')\n",
    "textcat.add_label('1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"Yes I pretty much agree with you Let's get rid of Experimental treatments and anything re transmission and the intro to the Treatment section as well but keep the Level of care section But even still that will not reduce the article nearly as much as we need to reduce it I am constantly going over all of the info compressing it but IMO if we do much more of that we will lose what the article is all about and devote more and more to the extremely lengthy and bound to grow Response section which other than the international organizations I doubt is not of much interest to most other than what their own country is doing The last time I looked the article was more than three times the recommended length\",\n",
       "  {'cats': {'0': True, '1': False}}),\n",
       " (\"Not a country Good catch I've removed Puerto Rico And I've got no objection to the page being renamed in fact I'm almost certain that somebody will rename it if it survives VfD Thanks for the feedback and I'll try to get your name right next time\",\n",
       "  {'cats': {'0': True, '1': False}}),\n",
       " ('Hey Seth you are a fucking douchebag', {'cats': {'0': False, '1': True}})]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#создаю список кортежей для облегчения перемешивания от эпохи к эпохе\n",
    "train_texts = X_train_scapy.values\n",
    "train_labels = [{'cats':{'0': label == 0, '1': label == 1}} for label in y_train_scapy.values]\n",
    "train_data = list(zip(train_texts, train_labels))\n",
    "train_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "843d368958364d1fa58158e8ed835e54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=159.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'parser': 0.0, 'tagger': 965.7055566310883, 'ner': 0.0, 'textcat': 0.47771043042182426}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b5a26f3ee246159344ef61e82a4a54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=159.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'parser': 0.0, 'tagger': 1390.4857531348243, 'ner': 0.0, 'textcat': 0.9147957504171934}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "389eb837711640dc9888c11a5f23646b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=159.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'parser': 0.0, 'tagger': 1544.0681671695784, 'ner': 0.0, 'textcat': 1.359043264439717}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b74f5e554a654d2c8ca4c4b6f4ba0200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=159.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'parser': 0.0, 'tagger': 1671.6116955066682, 'ner': 0.0, 'textcat': 1.768525060608409}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98fa57e110f84e66bc5b8fd6b4f8f24d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=159.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'parser': 0.0, 'tagger': 1749.0042165439809, 'ner': 0.0, 'textcat': 2.196045166662777}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cd99fa461f948ebafcb873984ebc857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=159.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'parser': 0.0, 'tagger': 1832.1568360478268, 'ner': 0.0, 'textcat': 2.5717079027639755}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d91a8f6745f443ab8a610b4c9e345f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=159.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'parser': 0.0, 'tagger': 1900.0533651040314, 'ner': 0.0, 'textcat': 2.931857637533127}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c7acb2e7d141828ba6f045a7ca6ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=159.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'parser': 0.0, 'tagger': 1969.8723933807341, 'ner': 0.0, 'textcat': 3.2188576189279843}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38db6932fce246339e2c5b1718148e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=159.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'parser': 0.0, 'tagger': 2035.9629731759342, 'ner': 0.0, 'textcat': 3.406117173124341}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c4502d094c474bb466e8e2f8bcfbd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=159.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'parser': 0.0, 'tagger': 2095.048223697726, 'ner': 0.0, 'textcat': 3.541731536155109}\n",
      "Wall time: 8min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#подключаю random для обучения по эпохам\n",
    "random.seed(1)\n",
    "spacy.util.fix_random_seed(1)\n",
    "#оптимизатор для обновления модели\n",
    "optimizer = nlp.begin_training()\n",
    "\n",
    "batch_size = 8\n",
    "losses = {}\n",
    "\n",
    "#обучаю эпохами\n",
    "for epoch in range(10):\n",
    "    \n",
    "    #перемешиваю данные\n",
    "    random.shuffle(train_data)\n",
    "    \n",
    "    #функция возвращает генератор минибатчей для обучения\n",
    "    batches_gen = minibatch(train_data, size=batch_size)\n",
    "\n",
    "    #число батчей (для счётчика)\n",
    "    n_batches = len(train_data) // batch_size + 1 - 0**(len(train_data) % batch_size)\n",
    "    \n",
    "    #иду по батчам\n",
    "    for batch, i in zip(batches_gen, notebook.tqdm(range(n_batches-1))):\n",
    "        #разделяем минибатчи на текст и метки\n",
    "        texts, labels = zip(*batch)\n",
    "        #обновляем параметры модели\n",
    "        nlp.update(texts, labels, sgd=optimizer, losses=losses)\n",
    "    print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#здесь была запись готовой модели на диск, но я решил не перегружать проект импортом ещё и моделей\n",
    "#nlp.to_disk(\"/spacy_nlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112628    Ok thanks '''''' Your GA nomination of Basehea...\n",
       "142604    Go to hell You won't be laughing after I conta...\n",
       "18356     Question Hi I saw you responded to my post abo...\n",
       "75086                                in the areas marked as\n",
       "36038     Amen my sock pupinski brother Hack a long plea...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_scapy[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применяю пайп к тестовому набору. <p style=\"color:IndianRed\"> Вроде бы здесь верно? на [kaggle](https://www.kaggle.com/matleonard/text-classification) применяют только токенизатор, но там пустая модель, без других элементов пайплайна. Или надо отключать textcat перед применением пайпа?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Ok thanks '''''' Your GA nomination of Basehead The article Basehead you nominated as a good article has been placed on hold The article is close to meeting the good article criteria but there are some minor changes or clarifications needed to be addressed If these are fixed within seven days the article will pass otherwise it will fail See Talk Basehead for things which need to be addressed,\n",
       " Go to hell You won't be laughing after I contact the sysops to revert your vandalism Have a nice day you transmisogynist fuck CFE FA E C DE C,\n",
       " Question Hi I saw you responded to my post about taking it up with an ANI MY question is what is an ANI Thanks,\n",
       " in the areas marked as,\n",
       " Amen my sock pupinski brother Hack a long please leave my links and quit being a mini dictator ThANKS]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#применяю пайп к тестовому набору\n",
    "test_docs = list(nlp.pipe(X_test_scapy))\n",
    "test_docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #токенизирую тестовый набор признаков - это если загружена пустая модель nlp = spacy.blank(\"en\")\n",
    "# X_test_tokenized = [nlp.tokenizer(text) for text in X_test_scapy]\n",
    "# X_test_tokenized[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.9155748e-01 1.4475163e-03]\n",
      " [7.7116221e-02 9.2308694e-01]\n",
      " [9.9860686e-01 1.8904857e-04]\n",
      " [9.9956137e-01 4.5397872e-05]\n",
      " [2.5371355e-03 9.9893397e-01]]\n"
     ]
    }
   ],
   "source": [
    "#вызываю обученный категоризатор из пайпа\n",
    "textcat = nlp.get_pipe('textcat')\n",
    "scores, _ = textcat.predict(test_docs)\n",
    "print(scores[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1]\n",
      "('0', '1')\n"
     ]
    }
   ],
   "source": [
    "#проверяю, что метки соответствуют своим номерам\n",
    "predicted_labels_num = scores.argmax(axis=1)\n",
    "print(predicted_labels_num[:5])\n",
    "print(textcat.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted NO</th>\n",
       "      <th>Predicted YES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual NO</th>\n",
       "      <td>275</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual YES</th>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Predicted NO  Predicted YES\n",
       "Actual NO            275             13\n",
       "Actual YES            13             19"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------report---------------------------\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95       288\n",
      "           1       0.59      0.59      0.59        32\n",
      "\n",
      "    accuracy                           0.92       320\n",
      "   macro avg       0.77      0.77      0.77       320\n",
      "weighted avg       0.92      0.92      0.92       320\n",
      "\n",
      "ROC_AUC: 0.77\n"
     ]
    }
   ],
   "source": [
    "print_report(y_test_scapy, predicted_labels_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:SteelBlue; font-size:200%; line-height:1.5\">4. BERT + LogisticRegression</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При работе с BERT на локальной машине получил эмбеддинги для всего набора данных. Но файл с ними в формате *.npy* весил 936 МБ, поэтому их проблематично включить в работу. Поэтому ниже продемонстрирую в коде, как работал с BERT.\n",
    "\n",
    "Важным этапом была оптимизация набора данных согласно [Smart batchin tutorial](https://colab.research.google.com/drive/1Er23iD96x_SzmRG8md1kVggbmz0su_Q5#scrollTo=Sa7tgw2-Pttq). Суть в том, чтобы упорядочить текстовые вектора выборки по возрастанию длины, потом разбить их на батчи и для каждого батча делать свой пэддинг. Это подходит для BERT и позволяет сильно сократить время и объём памяти, т.к. большая часть нашего набора данных - короткие текстовые вектора. Если делать паддинг общий по всему датасету, мы искусственно раздуваем эти вектора в десятки раз.\n",
    "\n",
    "Это действительно работает и позволило получить эмбеддинги для нашего датасета на cpu примерно за 5 часов. Один раз процесс упал из-за недостатка памяти, но я включил в цикл построчную запись векторов из пакетов эмбеддинга в csv-файл, поэтому данные не потерялись (в коде закомментил). Чтобы иметь возможность проверить, что после упорядочивания и последующей склейки эмбеддингов из батчей ничего не перепуталось, я также упорядочивал и склеивал вместе с ними метки и индексы изначального датасета. В итоге это помогло мне после падения ядра при проверке порядка батчей. Оставил это в коде, хотя для маленькой выборки можно часть действий убрать. Также в коде закомментил построчную запись в файл csv.\n",
    "\n",
    "Результат на эмбеддингах для полного датасета получился хуже, чем для комбинации tfidf+logreg (см. изображения в конце раздела)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# токенизирует текст\n",
    "def text_tokenizer(tokenizer, corpus):\n",
    "       \n",
    "    tokenized_corpus = []\n",
    "    for i in notebook.tqdm( range(0, len(corpus)) ):  \n",
    "        tokenized_corpus.append(tokenizer.encode(corpus[i], \n",
    "                                                 add_special_tokens=True, \n",
    "                                                 max_length=400, \n",
    "                                                 truncation=True,\n",
    "                                                 padding=False))\n",
    "    return tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#определение максимальной длины вектора текста в токенизированном корпусе/батче\n",
    "def find_max_len(tokenized_corpus):\n",
    "    \n",
    "    max_len = 0\n",
    "    for vector in tokenized_corpus:\n",
    "        if len(vector) > max_len:\n",
    "            max_len = len(vector)\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding векторов текста из токенизированного корпуса/батча\n",
    "def vectors_padding(tokenized_corpus):\n",
    "    \n",
    "    max_len = find_max_len(tokenized_corpus)\n",
    "    v = np.array([0 for i in range(0, max_len)])\n",
    "    \n",
    "    \n",
    "    for k in range(0, len(tokenized_corpus)):\n",
    "        tokenized_corpus[k] = np.hstack (( tokenized_corpus[k], v[ len(tokenized_corpus[k]): ] ))\n",
    "       \n",
    "    return np.array(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dl, _, y_dl, _, ind_dl, _ = \\\n",
    "train_test_split(df['text'].values, \n",
    "                 df['toxic'].values, \n",
    "                 df.index, \n",
    "                 stratify = df['toxic'].values, \n",
    "                 train_size = 0.01, random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config type: <class 'transformers.tokenization_bert.BertTokenizer'> \n",
      "\n",
      "Config type: <class 'transformers.configuration_bert.BertConfig'> \n",
      "\n",
      "Model type: <class 'transformers.modeling_bert.BertModel'>\n"
     ]
    }
   ],
   "source": [
    "pretrained_weights = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_weights)\n",
    "\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name_or_path=pretrained_weights)\n",
    "\n",
    "config.output_hidden_states=True\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    pretrained_model_name_or_path='bert-base-uncased',\n",
    "    config=config)\n",
    "\n",
    "print('Config type:', str(type(tokenizer)), '\\n')\n",
    "print('Config type:', str(type(config)), '\\n')\n",
    "print('Model type:', str(type(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ad19447742d426b9d7df8405ab8a30f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1595.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[101, 1999, 2755, 2023, 3849, 2000, 2022, 1996, 10465, 1998, 2109, 2011, 1996, 3484, 1997, 5470, 4680, 7201, 2062, 2059, 20851, 20056, 2182, 102] \n",
      "\n",
      "[101, 5292, 2232, 2008, 1005, 1055, 3243, 10303, 1045, 6669, 3305, 2589, 2009, 1037, 2261, 2335, 2870, 1005, 1005, 1005, 1005, 1005, 1005, 2831, 2000, 1996, 2192, 102] \n",
      "\n",
      "[101, 4312, 9391, 2157, 2030, 7929, 2003, 1996, 2168, 2004, 2108, 2157, 2049, 1041, 2350, 102] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized = text_tokenizer(tokenizer, X_dl)\n",
    "for token in tokenized[:3]:\n",
    "    print(token,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([101, 2089, 102], 0, 177), ([101, 2257, 102], 0, 16312), ([101, 2238, 102], 0, 136419)]\n",
      "Shortest sample: 3\n",
      "Longest sample: 400\n"
     ]
    }
   ],
   "source": [
    "# совместная сортировка списков по длине текста\n",
    "train_samples = sorted(zip(tokenized, y_dl, ind_dl), key=lambda x: len(x[0]))\n",
    "print(train_samples[:3])\n",
    "print('Shortest sample:', len(train_samples[0][0]))\n",
    "print('Longest sample:', len(train_samples[-1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid', font_scale=1.1)\n",
    "plt.rcParams[\"figure.figsize\"] = (8,4)\n",
    "sorted_lengths = [len(s[0]) for s in train_samples]\n",
    "plt.plot(range(0, len(sorted_lengths)), sorted_lengths)\n",
    "plt.xlabel('номер объекта')\n",
    "plt.ylabel('длина комментария')\n",
    "plt.title('Объекты после сортировки')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот здеь хорошо видно, что большая часть датасета - короткие текстовые вектора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d36fd953ba184f83a4cc0ec06926d976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "#кроме текстов я собираю метки и индексы, чтобы сопоставить упорядоченные эмбеддинги изначальному датасету\n",
    "#и это очень пригодилось, когда процесс упал с ошибкой\n",
    "batch_ordered_texts = []\n",
    "batch_ordered_labels = []\n",
    "batch_ordered_indexes = []\n",
    "\n",
    "range_upper_limit = 1 + len(train_samples)//batch_size\n",
    "\n",
    "#иду по части train_samples, размер которой кратен числу пакетов\n",
    "for i in notebook.tqdm( range(0, range_upper_limit) ) :\n",
    "    \n",
    "    #выбираю пакет\n",
    "    batch = train_samples[ i*batch_size : (i+1)*batch_size ]\n",
    "    \n",
    "    #каждый объект в пакете - это кортеж из токенизированного текста, метки и изначального индекса\n",
    "    #разделяю их\n",
    "    batch_ordered_texts.append([s[0] for s in batch])\n",
    "    batch_ordered_labels.append([s[1] for s in batch])\n",
    "    batch_ordered_indexes.append([s[2] for s in batch])\n",
    "\n",
    "#удаляю train samples, чтобы разгрузить память\n",
    "del train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0]]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_ordered_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100, 100)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_ordered_texts), len(batch_ordered_labels), len(batch_ordered_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f7ba15b07b542e0ab20857d6fd1868c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_padded_texts = []\n",
    "batch_attn_masks = []\n",
    "\n",
    "for i in notebook.tqdm( range(0, len(batch_ordered_texts)) ):\n",
    "    batch = batch_ordered_texts[i]\n",
    "    padded = vectors_padding(batch)\n",
    "    attention_mask = np.where(padded != 0, 1, 0)\n",
    "    batch_padded_texts.append(padded)\n",
    "    batch_attn_masks.append(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  101,  9152,  3215,  2818,  3489,  2023,  2003,  1996,  4957,\n",
       "         2000,  3319,   102,     0],\n",
       "       [  101,  1999,  1998,  2069, 24501, 27082,  2094,  6822,  2008,\n",
       "         5038,  1999,   102,     0],\n",
       "       [  101,  8840,  2140,  1051, 11923,  1045,  3214,  2026, 27571,\n",
       "         2546,  2831,   102,     0],\n",
       "       [  101,  2017,  2031,  2042,  8534,  2005,  2847,  2831,  9530,\n",
       "        18886,  5910,   102,     0],\n",
       "       [  101,  2128, 16009,  3431,  6616,  2125, 10041,  5310,  3147,\n",
       "        12618,  8322,   102,     0],\n",
       "       [  101,  2233, 11396,  2808,  1997,  4623,  2024,  2025,  8152,\n",
       "         8182,  2831,   102,     0],\n",
       "       [  101,  1045,  3305,  2085,  1045,  2097,  3191,  2013,  1996,\n",
       "         2518,  2100,   102,     0],\n",
       "       [  101,  2417,  7442,  6593,  2831,  2358,  7723,  1005,  1055,\n",
       "         2902, 28425,   102,     0],\n",
       "       [  101,  1045,  2031,  3880,  2000,  2115,  5227,  2006,  2115,\n",
       "         2831,  3931,   102,     0],\n",
       "       [  101,  3395,  2073,  2115,  3947,  2097,  2022,  6179,  1998,\n",
       "         2092,  2363,   102,     0],\n",
       "       [  101,  6616,  2125,  6616,  2017,  2017, 15536,  3211,  6904,\n",
       "        13871,  4140,   102,     0],\n",
       "       [  101,  2156,  1996,  1050,  2094,  6302,  3597, 14756,  2094,\n",
       "         3931,  1997,   102,     0],\n",
       "       [  101,  2008,  2001,  4895,  9289,  3709,  2005,  2017,  2031,\n",
       "         2042,  7420,   102,     0],\n",
       "       [  101,  1045,  2031,  2657,  1996,  2744,  2077,  2021,  2049,\n",
       "         8192,  2003,  4678,   102],\n",
       "       [  101,  2079,  2017,  9544,  2000,  2562,  1996,  5762,  1998,\n",
       "        24004, 20200,  3430,   102],\n",
       "       [  101,  2644,  9260,  2061,  2011,  2437,  2023, 10086,  2009,\n",
       "         2097,  2644,  2551,   102]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(batch_padded_texts[10])\n",
    "display(batch_attn_masks[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_padded_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2203cd3f09524df59f04888c2ed66838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wall time: 1min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embeddings = [] #при записи в файл данная строка не нужна\n",
    "for i in notebook.tqdm( range(0, len(batch_padded_texts)) ):\n",
    "    batch = torch.LongTensor(batch_padded_texts[i]) \n",
    "    mask_batch = torch.LongTensor(batch_attn_masks[i])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        batch_embeddings = model(batch, attention_mask=mask_batch)\n",
    "\n",
    "    embeddings.append(batch_embeddings[0][:,0,:].numpy()) #при записи в файл данная строка не нужна\n",
    "#     with open('embeddings_light.csv', 'a') as fp:\n",
    "#         for row in batch_embeddings[0][:,0,:].numpy():\n",
    "#             csv.writer(fp, delimiter=\",\").writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1595, 1595)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_bert = []\n",
    "ind_bert = []\n",
    "for labels in batch_ordered_labels:\n",
    "    y_bert += labels\n",
    "\n",
    "for ind in batch_ordered_indexes:\n",
    "    ind_bert += ind\n",
    "    \n",
    "len(y_bert), len(ind_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df = pd.DataFrame(np.concatenate(embeddings))#при записи в файл данная строка не нужна\n",
    "#embeddings_df = pd.read_csv('embeddings_light.csv', header = None) #при записи в файл раскомментировать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1595"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>-0.702640</td>\n",
       "      <td>0.189393</td>\n",
       "      <td>-0.078365</td>\n",
       "      <td>-0.271436</td>\n",
       "      <td>-0.656767</td>\n",
       "      <td>0.253748</td>\n",
       "      <td>0.102763</td>\n",
       "      <td>1.084641</td>\n",
       "      <td>-0.165862</td>\n",
       "      <td>-0.425184</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.354917</td>\n",
       "      <td>0.022897</td>\n",
       "      <td>-0.729087</td>\n",
       "      <td>0.407214</td>\n",
       "      <td>-0.549863</td>\n",
       "      <td>-0.610985</td>\n",
       "      <td>-0.119268</td>\n",
       "      <td>0.443020</td>\n",
       "      <td>0.361566</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>-0.566810</td>\n",
       "      <td>0.109878</td>\n",
       "      <td>0.141035</td>\n",
       "      <td>-0.393483</td>\n",
       "      <td>0.152481</td>\n",
       "      <td>-0.276171</td>\n",
       "      <td>0.212249</td>\n",
       "      <td>0.382914</td>\n",
       "      <td>-0.132682</td>\n",
       "      <td>-0.244495</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031573</td>\n",
       "      <td>0.365090</td>\n",
       "      <td>0.031150</td>\n",
       "      <td>-0.283040</td>\n",
       "      <td>-0.144459</td>\n",
       "      <td>-0.085462</td>\n",
       "      <td>-0.329101</td>\n",
       "      <td>0.473700</td>\n",
       "      <td>0.125089</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>0.098102</td>\n",
       "      <td>0.005101</td>\n",
       "      <td>-0.082653</td>\n",
       "      <td>-0.026029</td>\n",
       "      <td>-0.160023</td>\n",
       "      <td>-0.518725</td>\n",
       "      <td>0.447245</td>\n",
       "      <td>0.497125</td>\n",
       "      <td>0.065516</td>\n",
       "      <td>-0.177982</td>\n",
       "      <td>...</td>\n",
       "      <td>0.242854</td>\n",
       "      <td>0.343632</td>\n",
       "      <td>-0.077081</td>\n",
       "      <td>-0.060330</td>\n",
       "      <td>0.032362</td>\n",
       "      <td>-0.057464</td>\n",
       "      <td>-0.247976</td>\n",
       "      <td>0.429186</td>\n",
       "      <td>0.729174</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>-0.199827</td>\n",
       "      <td>0.572571</td>\n",
       "      <td>-0.321351</td>\n",
       "      <td>0.022956</td>\n",
       "      <td>-0.296194</td>\n",
       "      <td>-0.057194</td>\n",
       "      <td>0.678060</td>\n",
       "      <td>0.589858</td>\n",
       "      <td>-0.523916</td>\n",
       "      <td>-0.498981</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051718</td>\n",
       "      <td>0.006724</td>\n",
       "      <td>0.204458</td>\n",
       "      <td>-0.021316</td>\n",
       "      <td>-0.119137</td>\n",
       "      <td>0.281101</td>\n",
       "      <td>-0.519684</td>\n",
       "      <td>0.526997</td>\n",
       "      <td>0.472728</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>-0.299016</td>\n",
       "      <td>-0.262969</td>\n",
       "      <td>0.241281</td>\n",
       "      <td>-0.127563</td>\n",
       "      <td>-0.224833</td>\n",
       "      <td>-0.182682</td>\n",
       "      <td>0.545353</td>\n",
       "      <td>0.548591</td>\n",
       "      <td>-0.161481</td>\n",
       "      <td>-0.220912</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.120513</td>\n",
       "      <td>0.091539</td>\n",
       "      <td>0.154801</td>\n",
       "      <td>0.271337</td>\n",
       "      <td>-0.151544</td>\n",
       "      <td>-0.104738</td>\n",
       "      <td>-0.543858</td>\n",
       "      <td>0.372585</td>\n",
       "      <td>0.571242</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "126 -0.702640  0.189393 -0.078365 -0.271436 -0.656767  0.253748  0.102763   \n",
       "177 -0.566810  0.109878  0.141035 -0.393483  0.152481 -0.276171  0.212249   \n",
       "215  0.098102  0.005101 -0.082653 -0.026029 -0.160023 -0.518725  0.447245   \n",
       "248 -0.199827  0.572571 -0.321351  0.022956 -0.296194 -0.057194  0.678060   \n",
       "314 -0.299016 -0.262969  0.241281 -0.127563 -0.224833 -0.182682  0.545353   \n",
       "\n",
       "            7         8         9  ...       759       760       761  \\\n",
       "126  1.084641 -0.165862 -0.425184  ... -0.354917  0.022897 -0.729087   \n",
       "177  0.382914 -0.132682 -0.244495  ...  0.031573  0.365090  0.031150   \n",
       "215  0.497125  0.065516 -0.177982  ...  0.242854  0.343632 -0.077081   \n",
       "248  0.589858 -0.523916 -0.498981  ...  0.051718  0.006724  0.204458   \n",
       "314  0.548591 -0.161481 -0.220912  ... -0.120513  0.091539  0.154801   \n",
       "\n",
       "          762       763       764       765       766       767  toxic  \n",
       "126  0.407214 -0.549863 -0.610985 -0.119268  0.443020  0.361566      0  \n",
       "177 -0.283040 -0.144459 -0.085462 -0.329101  0.473700  0.125089      0  \n",
       "215 -0.060330  0.032362 -0.057464 -0.247976  0.429186  0.729174      0  \n",
       "248 -0.021316 -0.119137  0.281101 -0.519684  0.526997  0.472728      0  \n",
       "314  0.271337 -0.151544 -0.104738 -0.543858  0.372585  0.571242      0  \n",
       "\n",
       "[5 rows x 769 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#изначально я предполагал сравнивать с предсказаниями других моделей на их тесте,\n",
    "#поэтому собирал датасет и упорядочивал его согласно изначальным индексам\n",
    "#а затем делил на np.array\n",
    "#возможно, не самый оптимальный подход, но помог при падении ядра\n",
    "embeddings_df.loc[:, 'toxic'] = y_bert\n",
    "embeddings_df.index = ind_bert\n",
    "embeddings_df.sort_index(inplace=True)\n",
    "#на полном датасете было около 300 nan, возможно, в связи с падением процесса\n",
    "embeddings_df.fillna(0, inplace = True)\n",
    "embeddings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bert = embeddings_df.drop(columns=['toxic']).values\n",
    "y_bert = embeddings_df['toxic']\n",
    "del embeddings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bert_train, X_bert_test, y_bert_train, y_bert_test = \\\n",
    "train_test_split(X_bert, y_bert, test_size=0.1, stratify = y_bert, random_state = 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 951 epochs took 11 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   11.4s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted NO</th>\n",
       "      <th>Predicted YES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual NO</th>\n",
       "      <td>139</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual YES</th>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Predicted NO  Predicted YES\n",
       "Actual NO            139              5\n",
       "Actual YES            10              6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------report---------------------------\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95       144\n",
      "           1       0.55      0.38      0.44        16\n",
      "\n",
      "    accuracy                           0.91       160\n",
      "   macro avg       0.74      0.67      0.70       160\n",
      "weighted avg       0.89      0.91      0.90       160\n",
      "\n",
      "ROC_AUC: 0.67\n",
      "Wall time: 11.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logreg = LogisticRegression(solver='saga', \n",
    "                            max_iter=10000, \n",
    "                            class_weight={1:2, 0:1}, \n",
    "                            C=5,\n",
    "                            verbose=1)\n",
    "logreg.fit(X_bert_train, y_bert_train)\n",
    "y_bert_test_predicted  = logreg.predict(X_bert_test)\n",
    "print_report(y_bert_test, y_bert_test_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Данные и результаты BERT на полном наборе данных:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://getfile.dokpub.com/yandex/get/https://yadi.sk/i/5e1x65QX9nX0mg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://getfile.dokpub.com/yandex/get/https://yadi.sk/i/oamREYdN1mNs8Q\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:SteelBlue; font-size:200%; line-height:1.5\">Вывод</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучше всего себя показала комбинация:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 TfidfVectorizer(ngram_range=(1, 2),\n",
       "                                 stop_words={\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\",\n",
       "                                             \"'ve\", 'a', 'about', 'above',\n",
       "                                             'across', 'after', 'afterwards',\n",
       "                                             'again', 'against', 'ain', 'all',\n",
       "                                             'almost', 'alone', 'along',\n",
       "                                             'already', 'also', 'although',\n",
       "                                             'always', 'am', 'among', 'amongst',\n",
       "                                             'amount', 'an', 'and', 'another', ...})),\n",
       "                ('model',\n",
       "                 LogisticRegression(C=20, class_weight={0: 1, 1: 3.5},\n",
       "                                    max_iter=10000, solver='saga',\n",
       "                                    verbose=1))])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score 0.7817715019255456\n"
     ]
    }
   ],
   "source": [
    "print('f1_score', final_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На данном датасете BERT+LogisticRegression показал результат хуже. Есть большое количество вариантов, что попробовать ещё, но все они (большинство) требуют GPU в colab или чего-то аналогичного. Наверняка можно \"выжать\" больше из BERT. spaCy была только опробована на простой модели на маленькой выборке. Наверняка на полной выборке и более сложной модели результат будет гораздо лучше. Но по скорости + качество вполне достойный бейзлайн - TfidfVectorizer + LogisticRegression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
